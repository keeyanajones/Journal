<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Code Journal: Algorithms</title>   
    <!-- Latest compiled and minified CSS -->     
    <!-- Font Awesome 5.0.13 -->     
    <link rel="stylesheet" 
          href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" 
          integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" 
          crossorigin="anonymous">         
    <!-- Bootstrap 3.3.7 --> 
    <link rel="stylesheet" 
          href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" 
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" 
          crossorigin="anonymous">
    <!-- Highlight.js 9.13.1 -->     
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/androidstudio.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>    
    <script>hljs.initHighlightingOnLoad();</script>    
    
    <!-- Custom Style Pygment and Icons -->     
    <link rel="stylesheet" href="../assets/stylesheets/styles.css">
    <link rel="stylesheet" href="../assets/stylesheets/pygment_trac.css">
    <link rel="icon" href="../assets/images/favicon.ico" type="image/gif">
    <link rel="shortcut icon" href="../assets/images/favicon.png">    
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  
  <body>
    <div class="wrapper">               
    <!-- Header -->         
      <header>
          <img src="../assets/images/pb_Image.png" class="img-fluid" alt="me"/>
        <p class="view">A Code Journal for 
            <span class="fab fa-git-square" aria-hidden="true"></span>
                Hub Pages<br/>        
                
            <a href="https://github.com/keeyanajones/Journal" target="_blank">
            <span class="fab fa-github"></span> 
              View the Code Journal on GitHub</a></p>
                      
                        
            <button class="btn btn-md btn-default" type="button">
                <span class="fab fa-java fa-2x"></span> 
            </button>                
            
            <button class="btn btn-md btn-default" type="button">
                <span class="fab fa-python fa-2x"></span> 
            </button>                        

            <button class="btn btn-md btn-default" type="button">
                <span class="fab fa-r-project  fa-2x"></span> 
            </button>                
            
            <button class="btn btn-md btn-default" type="button">
            <span class="fab fa-js fa-2x"></span> 
            </button>                              

            <br/><br/>
            
            <a href="https://github.com/keeyanajones/">
            <small><span class="fab fa-github-alt"></span>                 
            Repositories and latest Contribution Activity</small></a>                 
              
        <br/>
        
        <!-- Download, Clone, or Fork -->
        <ul>    
          <li><a href="https://github.com/keeyanajones/Journal/archive/master.zip">
              <span class="fa fa-file-archive" aria-hidden="true"></span><br/>
                  Download <br/> ZIP</a></li>                  
          <li><a href="https://github.com/keeyanajones/Journal">
              <span class="fa fa-clone" aria-hidden="true"></span><br/>
                  Clone <br/> Desktop</a></li>                  
          <li><a href="https://github.com/keeyanajones/Journal">
              <span class="fa fa-code-branch" aria-hidden="true"></span><br/>
                  Fork On <br/> GitHub</a></li>
        </ul>                                 
      </header>
    <!-- ./header --> 
    </div>          
      
    <section>
        
          <h2><span class="fa fa-project-diagram " aria-hidden="true"></span>
              <a name="algroithms" class="anchor" href="#algorithms">
              <span class="header-link"></span></a>
            Algorithms <sup>in Java</sup></h2>                  
     
        <p><sup><span class="fa fa-calendar"></span>
                Last updated on May 6th, 2019</sup></p>
                                                 
        <ol class="breadcrumb">
          <li><span class="fa fa-bookmark"></span> 
              <a href="../index.html">Home</a></li>
          <li class="active"><a href="#algorithms">
              <span class="fa fa-bookmark"></span> Algorithms 
              <sup>in Java</sup></a></li>
        </ol>
        
        <p>Algorithms are a step-by-step procedure, which defines a set of 
           instructions to be executed in a certain order to get the desired 
           output; generally created independent of underlying 
           languages, i.e. they can be implemented in more than one 
           programming language.</p>

        <p><sup>see <a href="problem-solving/index.html">Problem Solving</a> 
         to learn more...</sup></p>        
        
        <p>From the data structure point of view some important categories:</p>
        
        <p><sup>see <a href="../data-structures/index.html">Data Structures</a> 
         to learn more...</sup></p>

        <ul>
          <li>Search − search an item in a data structure
              (see <a href="searching/index.html">Searching</a>)</li>
          <li>Sort −  sort items in a certain order 
              (see <a href="sorting/index.html">Sorting</a>)</li>
          <li>Insert − insert item in a data structure
              (see <a href="shuffling/index.html">Shuffling</a>)</li>
          <li>Update − update an existing item in a data structure</li>
          <li>Delete − delete an existing item from a data structure</li>
        </ul>
        
          <p><sup>see <a href="graphs/index.html">Graphs</a>,
                      <a href="trees/index.html">Trees</a>, and 
                      <a href="recursion/index.html">Recursion</a> to 
                  learn more...
            </sup>
          </p>            
        <br/>

        <a name="characteristics" class="anchor" href="#characteristics"></a>
        <h3>Characteristics</h3>
        <p>Not all procedures can be called an algorithm; it should 
           have the following characteristics:</p>

        <ul>
          <li>Unambiguous − be clear and unambiguous. Each of 
                            its steps (or phases), and their inputs/outputs 
                            clear and must lead to only one meaning.</li>
          <li>Input −  have 0 or more well-defined 
                      inputs.</li>
          <li>Output − have 1 or more well-defined outputs, 
                       and match the desired output.</li>
          <li>Finiteness − must terminate after a finite number of steps.</li>
          <li>Feasibility − feasible with the available resources.</li>
          <li>Independent − have step-by-step directions, which is independent 
                            of any programming code.</li>
        </ul>

        <p>Algorithm writing is a process and is executed after the problem 
           domain is well-defined; knowing the problem domain, 
           for which we are designing a solution.</p>
 
        <h4>EXAMPLE</h4>
        <p>PROBLEM − Design an algorithm to add two numbers and display the 
                     result.</p>

<pre>
<code>
Step 1 − START
Step 2 − declare three integers a, b &amp; c
Step 3 − define values of a &amp; b
Step 4 − add values of a &amp; b
Step 5 − store output of step 4 to c
Step 6 − print c
Step 7 − STOP
</code>
</pre>

<p>Alternatively, it can be written as:</p>

<pre>
<code>
Step 1 − START ADD
Step 2 − get values of a & b
Step 3 − c ← a + b
Step 4 − display c
Step 5 − STOP
</code>
</pre>

        <p>In design and analysis of algorithms, usually the second method is 
           used to describe an algorithm. It makes it easy for the analyst to 
           analyze the algorithm ignoring all unwanted definitions. Observe what 
           operations are being used and how the process is flowing.</p>

        <p>Steps of algorithm design:</p>
          <ol>
            <li>Define the Problem</li>
            <li>Develop a model</li>
            <li>Specification of the algorithm</li>
            <li>Designing an algorithm</li>
            <li>Checking the correctness of the algorithm</li>
            <li>Analysis of algorithm</li>
            <li>Implementation of algorithm</li>
            <li>Program testing</li>
            <li>Documentation preparation</li>
          </ol>
        
        <p><sup>see <a href="problem-solving/index.html">Problem Solving</a> 
         to learn more...</sup></p>
        
        <p>Writing step numbers, is optional.</p>

        <p>Design an algorithm to get a solution of a given problem. A 
           problem can be solved in more than one ways.</p>

        <p>Hence, many solutions can be derived for a given problem. 
           The next step is to analyze those proposed algorithms and 
           implement the best suitable solution.</p>
        
        <a name="analysis" class="anchor" href="#analysis"></a>
        <h4>Analysis</h4>
        <p>Efficiency of can be analyzed at two different stages, 
           before and after implementation are the following:</p>

        <ul>
          <li>A Priori Analysis − This is a theoretical analysis of an 
              algorithm. Efficiency of it is measured by assuming 
              that all other factors, for example, processor speed, are 
              constant and have no effect on the implementation. It deals with 
              the execution or running time of various operations involved. The 
              running time of an operation can be defined as the number of 
              computer instructions executed per operation.</li>
            
          <li>A Posterior Analysis − This is an empirical analysis of an 
              algorithm. The selected algorithm is implemented using 
              programming language. This is then executed on target computer 
              machine. In this analysis, actual statistics like running time 
              and space required, are collected.</li>
        </ul>    
        
        <a name="complexity" class="anchor" href="#complexity"></a>
        <h4>Complexity</h4>
        <p>Suppose X is an algorithm and n is the size of input data, the time 
           and space used by the algorithm X are the two main factors, which 
           decide the efficiency of X.</p>

        <ul>
          <li>Time Factor − Time is measured by counting the number of key 
                            operations such as comparisons in the sorting 
                            algorithm.</li>
          <li>Space Factor − Space is measured by counting the maximum memory 
                             space required by the algorithm.</li>
        </ul>

        <p>The complexity of an algorithm f(n) gives the running time and/or the
           storage space required by the algorithm in terms of n as the size of 
           input data.</p>
        
        <a name="space-complexity" class="anchor" href="#space-complexity"></a>
        <h4>Space Complexity</h4>
        <p>Space complexity of an algorithm represents the amount of memory 
           space required by the algorithm in its life cycle. The space required 
           by them is equal to the sum of the following two components:
        </p>
          
        <ul>    
          <li>A fixed part that is a space required to store certain data and 
              variables, that are independent of the size of the problem. For 
              example, simple variables and constants used, program size, 
              etc.</li>
          
          <li>A variable part is a space required by variables, whose size 
              depends on the size of the problem. For example, dynamic memory 
              allocation, recursion stack space, etc.</li>
        </ul>

        <p>Space complexity S(P) of any algorithm P is S(P) = C + SP(I), where 
           C is the fixed part and S(I) is the variable part of the algorithm, 
           which depends on instance characteristic I. Following is a simple 
           example that tries to explain the concept:</p>
<pre>
<code>
Algorithm: SUM(A, B)
Step 1 -  START
Step 2 -  C ← A + B + 10
Step 3 -  Stop
</code>
</pre>
        
        <p>Here we have three variables A, B, and C and one constant. Hence 
           S(P) = 1 + 3. Now, space depends on data types of given variables 
           and constant types and it will be multiplied accordingly.</p>

        <a name="time-complexity" class="anchor" href="#time-complexity"></a>        
        <h4>Time Complexity</h4>
        <p>Time complexity of an algorithm represents the amount of time 
           required by the algorithm to run to completion. Time requirements 
           can be defined as a numerical function T(n), where T(n) can be 
           measured as the number of steps, provided each step consumes constant
           time.</p>

        <p>For example, addition of two n-bit integers takes n steps. 
           Consequently, the total computational time is T(n) = c ∗ n, where c 
           is the time taken for the addition of two bits. Here, observe 
           that T(n) grows linearly as the input size increases.</p>
          
        <a name="asymptotic-analysis" class="anchor" href="#asymptotic-analysis"></a>
        <h3>Asymptotic Analysis</h3>
        <p>Asymptotic analysis of an algorithm refers to defining the 
           mathematical boundation/framing of its run-time performance. Using 
           asymptotic analysis, conclude the best case, 
           average case, and worst case scenario of an algorithm.</p>

        <p>It is input bound i.e., if there's no input to the 
           algorithm, it is concluded to work in a constant time. Other than 
           the "input" all other factors are considered constant.</p>

        <p>It also refers to computing the running time of any 
           operation in mathematical units of computation. For example, the 
           running time of one operation is computed as f(n) and may be for 
           another operation it is computed as g(n2). This means the first 
           operation running time will increase linearly with the increase in n 
           and the running time of the second operation will increase 
           exponentially when n increases. Similarly, the running time of both 
           operations will be nearly the same if n is significantly small.</p>

        <p>Usually, the time required by an algorithm falls under three types:
            
        <ul>    
          <li>Best Case − Minimum time required for program execution.</li>
          <li>Average Case − Average time required for program execution.</li>
          <li>Worst Case − Maximum time required for program execution.</li>
        </ul>

        <a name="asymptotic-notations" class="anchor" href="#asymptotic-notations"></a>        
        <h4>Asymptotic Notations</h4>
        <p>Following are the commonly used asymptotic notations to calculate 
           the running time complexity of an algorithm.</p>

        <ul>
          <li>Ο Notation</li>
          <li>&Omega; Notation</li>
          <li>&theta; Notation</li>
        </ul>

        <a name="big-o-notation" class="anchor" href="#big-o-notations"></a>        
        <h4>Big Oh Notation, Ο</h4>
        <p>The notation Ο(n) is the formal way to express the upper bound of an 
           algorithm's running time. It measures the worst case time complexity 
           or the longest amount of time an algorithm can possibly take to 
           complete.</p>
       
        <p><sup>see <a href="../big-o-notations/index.html">Big O'Notations</a> 
         to learn more...</sup></p>
        
        For example, for a function f(n)

<pre>
<code>
Ο(f(n)) = { g(n) : there exists c > 0 

and

n0 such that f(n) ≤ c.g(n) for all n > n0. }
</code>
</pre>

        <a name="omega-notation" class="anchor" href="#omega-notation"></a>  
        <h4>Omega Notation, &Omega;</h4>

        <p>The notation Ω(n) is the formal way to express the lower bound of an 
           algorithm's running time. It measures the best case time complexity 
           or the best amount of time an algorithm can possibly take to 
           complete.</p>

        <p><sup>see <a href="../big-o-notations/index.html">Big O'Notations</a> 
         to learn more...</sup></p>
        
        For example, for a function f(n)

<pre>
<code>
Ω(f(n)) ≥ { g(n) : there exists c > 0 

and 

n0 such that g(n) ≤ c.f(n) for all n > n0. }
</code>
</pre>
           
        <a name="theta-notations" class="anchor" href="#theta-notation"></a>                
        <h4>Theta Notation, &theta;</h4>
        
        <p>The notation θ(n) is the formal way to express both the lower bound 
           and the upper bound of an algorithm's running time. It is represented
           as follows:</p>

        <p><sup>see <a href="../big-o-notations/index.html">Big O'Notations</a> 
         to learn more...</sup></p>
        
<pre>
<code>
θ(f(n)) = { g(n) if and only if g(n) =  Ο(f(n)) 

and 

g(n) = Ω(f(n)) for all n > n0. }
</code>
</pre>

        <a name="common-asymptotic-notations" class="anchor" href="#common-asymptotic-notations"></a>                
        <h4>Common Asymptotic Notations</h4>
        
<pre>
<code>
// common asymptotic notations:    
constant	Ο(1)
logarithmic	Ο(log n)
linear		Ο(n)
n log n		Ο(n log n)
quadratic	Ο(n2)
cubic		Ο(n3)
polynomial	nΟ(1)
exponential	2Ο(n)
</code>
</pre>        
        
        <h3>Greedy Algorithms</h3>        
        <p>An algorithm is designed to achieve optimum solution for a given 
           problem. In greedy algorithm approach, decisions are made from the 
           given solution domain. As being greedy, the closest solution that 
           seems to provide an optimum solution is chosen.</p>

        <p>Greedy algorithms try to find a localized optimum solution, which 
           may eventually lead to globally optimized solutions. However, 
           generally greedy algorithms do not provide globally optimized 
           solutions.</p>
               
        <h4>Counting Coins</h4>
        <p>This problem is to count to a desired value by choosing the least 
           possible coins and the greedy approach forces the algorithm to pick 
           the largest possible coin. If we are provided coins of  1, 2, 
           5 and 10 &cent; and we are asked to count 18 &cent; then the greedy 
           procedure will be:</p>

        <ol>        
          <li> − Select one 10 &cent; coin, the remaining count is 8</li>
          <li> − Then select one 5 &cent; coin, the remaining count is 3</li>
          <li> − Then select one 2 &cent; coin, the remaining count is 1</li>
          <li> − And finally, the selection of one 1 &cent; coins solves the 
                 problem</li>
        </ol>

        <p>Though, it seems to be working fine, for this count we need to pick 
           only 4 coins. But if we slightly change the problem then the same 
           approach may not be able to produce the same optimum result.</p>

        <p>For the currency system, where we have coins of 1, 7, 10 value, 
           counting coins for value 18 will be absolutely optimum but for count 
           like 15, it may use more coins than necessary. For example, the 
           greedy approach will use 10 + 1 + 1 + 1 + 1 + 1, total 6 coins. 
           Whereas the same problem could be solved by using only 3 coins 
           (7 + 7 + 1)</p>

        <p>Hence, we may conclude that the greedy approach picks an immediate 
           optimized solution and may fail where global optimization is a major 
           concern.</p>

        <p>Most networking algorithms use the greedy approach. Here is a list 
           of few of them:</p>

        <ul>
          <li>Traveling Salesman Problem</li>
          <li>Prim's Minimal Spanning Tree Algorithm</li>
          <li>Kruskal's Minimal Spanning Tree Algorithm</li>
          <li>Dijkstra's Minimal Spanning Tree Algorithm</li>
          <li>Graph - Map Coloring</li>
          <li>Graph - Vertex Cover</li>
          <li>Knapsack Problem</li>
          <li>Job Scheduling Problem</li>
        </ul>

        <p>There are lots of similar problems that uses the greedy approach to 
           find an optimum solution.</p>        

        <h4>Divide and Conquer</h4>    
        <p>In divide and conquer approach, the problem in hand, is divided into 
           smaller sub-problems and then each problem is solved independently. 
           When we keep on dividing the subproblems into even smaller 
           sub-problems, we may eventually reach a stage where no more division 
           is possible. Those "atomic" smallest possible sub-problem (fractions) 
           are solved. The solution of all sub-problems is finally merged in 
           order to obtain the solution of an original problem.</p>

        <p>Broadly, we can understand divide-and-conquer approach in a 
           three-step process.</p>

        <h4>Divide/Break</h4>
        <p>This step involves breaking the problem into smaller sub-problems. 
           Sub-problems should represent a part of the original problem. This 
           step generally takes a recursive approach to divide the problem 
           until no sub-problem is further divisible. At this stage, 
           sub-problems become atomic in nature but still represent some part 
           of the actual problem.</p>

        <h4>Conquer/Solve</h4>
        <p>This step receives a lot of smaller sub-problems to be solved. 
           Generally, at this level, the problems are considered 'solved' on 
           their own.</p>

        <h4>Merge/Combine</h4>
        <p>When the smaller sub-problems are solved, this stage recursively 
           combines them until they formulate a solution of the original 
           problem. This algorithmic approach works recursively and conquer 
           &amp; merge steps works so close that they appear as one.</p>

        <p>The following computer algorithms are based on divide-and-conquer 
           programming approach:</p>

        <ul>
          <li>Merge Sort
          <li>Quick Sort
          <li>Binary Search
          <li>Strassen's Matrix Multiplication
          <li>Closest pair (points)
        </ul>

        <p>There are various ways available to solve any computer problem, but 
           the mentioned are a good example of divide and conquer approach.</p>        

        <h4>Dynamic Programming</h4>
        <p>Dynamic programming approach is similar to divide and conquer in 
           breaking down the problem into smaller and yet smaller possible 
           sub-problems. But unlike, divide and conquer, these sub-problems 
           are not solved independently. Rather, results of these smaller 
           sub-problems are remembered and used for similar or overlapping 
           sub-problems.</p>

        <p>Dynamic programming is used where we have problems, which can be 
           divided into similar sub-problems, so that their results can be 
           re-used. Mostly, these algorithms are used for optimization. Before 
           solving the in-hand sub-problem, dynamic algorithm will try to 
           examine the results of the previously solved sub-problems. The 
           solutions of sub-problems are combined in order to achieve the best 
           solution.</p>

        <ul>
          <li>The problem should be able to be divided into smaller overlapping 
            sub-problem.</li>
          <li>An optimum solution can be achieved by using an optimum solution 
              of smaller sub-problems.</li>
          <li>Dynamic algorithms use memorization.</li>
        </ul>
        
        <h4>Comparison</h4>
        <p>In contrast to greedy algorithms, where local optimization is 
           addressed, dynamic algorithms are motivated for an overall 
           optimization of the problem.</p>

        <p>In contrast to divide and conquer algorithms, where solutions are 
           combined to achieve an overall solution, dynamic algorithms use the 
           output of a smaller sub-problem and then try to optimize a bigger 
           sub-problem. Dynamic algorithms use memorization to remember the 
           output of already solved sub-problems.</p>

        <p>The following computer problems can be solved using dynamic 
            programming approach:</p>

        <ul>
          <li>Fibonacci number series</li>
          <li>Knapsack problem</li>
          <li>Tower of Hanoi</li>
          <li>All pair shortest path by Floyd-Warshall</li>
          <li>Shortest path by Dijkstra</li>
          <li>Project scheduling</li>
        </ul>

        <p>Dynamic programming can be used in both top-down and bottom-up 
           manner. And of course, most of the times, referring to the previous 
           solution output is cheaper than recomputing in terms of CPU 
           cycles.</p>
                   
<hr/>

        <h3><a name="take-five" class="anchor" href="#take-five">
          <span class="header-link"></span></a>
          Review</h3>
          
        <p>Alright, let's recap what I've just learned:</p>
    
        <ul>
            <li>There are six characteristics of algorithms:</li>
            <ul>               
              <li>unambiguous, input, output, finiteness, feasibility, and 
                  independent </li>
            </ul>      
            
            <li>The two stages of algorithms analysis are priori and posterior</li>
            <li>The two factors of complexity are time and space</li>
            <li>Asymptotic analysis is defining math of run performance as best, 
                average, and worst cases</li>
            <li>The Big'O Notations are Omega and Theta</li>
            <li>Most networking algorithms use the greedy approach</li>
            <li>The divide and conquer approach divides problems into smaller
                pieces</li>            
            <li>Dynamic Programming divides problems into smaller pieces and 
                remembers and reuses simular results</li>
            <li>In comparison, greedy algorithms are locally optimized while
                dynamic algorithms are optimized overall</li>            
        </ul>
        
    <p>You can read more at: 
        <a href="https://go.java/" target="_blank">
           <small>https://go.java</small></a></p>

      <!-- pagination -->
       <a role="button" class="btn btn-primary pull-left"  
               href="../data-structures/index.html">
            &larr; PREVIOUS: Data Structures in Java</a>
          
      <a role="button" class="btn btn-primary pull-right" 
              href="../big-o-notations/index.html">
            NEXT: Big O'Notations &rarr;</a><br/><br/>
    </section>
        
    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" 
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" 
            crossorigin="anonymous"></script>         
    <script src="../assets/javascripts/scale.fix.js"></script>
  </body>
</html>

      